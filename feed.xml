<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://xeroblaze0.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xeroblaze0.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-25T03:14:59+00:00</updated><id>https://xeroblaze0.github.io/feed.xml</id><title type="html">blank</title><subtitle>Portfolio for Alex Hay | Robotics X Neuroscience </subtitle><entry><title type="html">Development of a Cribbage CPU Player</title><link href="https://xeroblaze0.github.io/blog/2023/cribbage-cpu/" rel="alternate" type="text/html" title="Development of a Cribbage CPU Player"/><published>2023-07-30T00:00:00+00:00</published><updated>2023-07-30T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2023/cribbage%20cpu</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2023/cribbage-cpu/"><![CDATA[<h2 id="cribbage">Cribbage</h2> <p>Cribbage is a 2+ person card game that uses a distinctive board to keep track of points. The game is played in hands with players discarding into a crib that alternates between counting towards each player’s score. Points are gained through card combinations in their hand and card combinations from play. The first player to 121 wins. It’s a great game to geek out on.</p> <div class="img_row"> <img class="col three" src="/assets/cribbage/Untitled.jpeg"/> </div> <p><br/></p> <h3 id="stats-gone-mad">Stats Gone Mad</h3> <p>I started with a python framework from <a href="https://github.com/jonathanmcmahon/cribbage">Github</a>. This repo seemed like a good base to start from; a few other people started from it and even corrected a scoring bug. A couple of interesting things to note: first, the deck is a object made of two dictionaries, one of suits and one of ranks, and the deck itself is a list of card objects. There’s a couple of deck-specific functions built into the class, like draw() and shuffle(). There’s two built in players, one a user controls, the other random. The random player uses no analysis for cribbing or playing cards but does provide an example of what the program expects.</p> <p>AI: Cribbage is a great game for stats and the nauances can make or break a game by a point. Every move and decision carries weight, and the tiniest details can spell the difference between triumph and defeat. Cribbage challenges players to master the art of precision and calculation. Whether it’s analyzing the probability of a winning hand or strategically discarding cards, the thrill lies not only in the gameplay itself, but also in the endless possibilities for statistical exploration and strategic decision-making.</p> <h3 id="building-out-the-cpu-player">Building out the CPU Player</h3> <p>The first thing I wanted to tackle was the decision process for discarding to the crib. The situation is static and really where all the analysis fun is. I started with evaluating starter card options. The program iterates through each hand + starter combination and tally’s up the hand score:</p> <div class="img_row"> <img class="col three" src="/assets/cribbage/Figure_01.png"/> </div> <p>In this instance we can see any 4 card combo-ing nicely with the pair of 5’s and the 6, for two 3-card straights and two 15’s. Likewise, 5’s 7’s and 9’s receive the same score through three-of-a-kind, straights, and combos of 15, respectively. This scenarios is 1 of 46 within 14 other possible hand combinations.</p> <h4 id="strategy">Strategy</h4> <p>The latitude by which one hand can vary provides a great field to “play ball” in. For instance if we look at the fact that 5, 7, and 9 would all provide the same score, they do not have the same likelihood of occuring. Our hand already has two 5’s so pulling another 5 is 2 in 46 (4.34%), while a 7 or 9 is twice as likely to occur with 4 in 46 (8.70%). Similarly 4 provides us with the highest score and has a 4 in 46 chance of occuring, however, every face card is also unaccounted for and (including the 10’s) has a 16 in 42 chance of occuring, almost 1 in 3. Amazing. What do these differences in scoring and likelihood mean?</p> <p>Well, we’ve really asked ourselves three-ish questions; what is the top score, what is the most probable, and what is the average?</p> <div class="img_row"> <img class="col three" src="/assets/cribbage/Figure_03.png"/> </div> <p>I consolidated the data into three 3D symmetrical graphs. They are symmetrical along the diagonal because you can’t discard a card you’ve already discarded. Behind the scenes I used permutations of each hand rather than combinations because it made the math behind the scenes easier.</p> <ul> <li>The <i>top score</i> is probably the easiest to understand. It’s the most optimistic, the “ooo, if I just get this card I win the game” mentality, but often not very likely to happen.</li> <li>The <i>most probable score</i> is the score you’ll most likely receive. As mentioned before all face cards (and 10’s) have a value of 10, meaning there are 16 cards with a value of 10, as opposed to just 4. Your most likely score will bias towards what those higher cards will give you, simply because there’s more higher value cards.</li> <li>The <i>average score</i> fills in the gap between top score and most likely, what I call a soft indicator of hand strength. It averages score across all combinations of hand and starter.</li> </ul> <p>Some of the math requires estimating the potential crib hand, which is unknowable. We can either disregard it or dive whole hog into stats, so lets get the wet suit on. <a href="http://www.cribbageforum.com/YourCrib.htm#addition">Discard tables</a> indicate the average number of points a crib will be worth if you discard a particular two-card combination and I included four in the program; Hessel, Colvert, Rasmussen, and Schell. Rasmussen is unique in that it was compiled using real world games rather than computer simulations. This helps capture some of the nuances and counter-plays that aren’t obvious even in play. The tables don’t agree on every detail — the composition of any crib is the result of a decision by two independent players, so it is impossible to arrive at absolute numbers. But taken together, they paint a good picture of what you can actually expect in your games.</p> <h3 id="outro--future-work">Outro &amp; Future Work</h3> <p>There’s a lot of places you can go armed with this information and a lot of different ways to play the game, but this is how I wrapped it up:</p> <p><code class="language-plaintext highlighter-rouge">return crib_permus[np.argmax(score_aves)]</code></p> <p>This returns the discard combo with the highest average score. The <i>top score</i> approach is too much of a silver bullet to be viable, and the <i>most likely</i> approach generally gives low scores (go figure). Highest average score does a good chunk of the leg work. Here’s test run against the RandomCPU player:</p> <div class="img_row"> <img class="col three" src="/assets/cribbage/Figure_04.png"/> </div> <p><br/> Winner!</p> <p>Future work would complete the other half of cribbage gameplay, pegging. Crib discard is a lot of stat analysis, graphing, and overall strategy of the hand. Pegging on the otherhand is all tactic, but necessarily depends on the discard. There are a few guidelines and situations to follow but it should be a degree more straight-forward than crib analysis.</p>]]></content><author><name></name></author><category term="math"/><category term="stats"/><category term="python"/><summary type="html"><![CDATA[Cribbage]]></summary></entry><entry><title type="html">Design Principals for NHP Laboratory Animals</title><link href="https://xeroblaze0.github.io/blog/2022/monkey_toys/" rel="alternate" type="text/html" title="Design Principals for NHP Laboratory Animals"/><published>2022-11-21T00:00:00+00:00</published><updated>2022-11-21T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2022/monkey_toys</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2022/monkey_toys/"><![CDATA[<h2 id="naxos-labs">Naxos Labs</h2> <p>In collaboration with a veterinarian, we started a company that designed a range of toys specifically tailored for NHPs and other laboratory animals. Through our experience we developed guidelines derived from valuable lessons, observations, and notes gathered during our interactions with the animals. <a href="https://naxoslab.ca/">Naxos Labs</a></p> <h3 id="purposeful-minimal-design">Purposeful Minimal Design</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/01-1400.webp"/> <img src="/assets/monkey/img/01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/02-1400.webp"/> <img src="/assets/monkey/img/02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/03-1400.webp"/> <img src="/assets/monkey/img/03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The toys were designed with simplicity in mind, featuring intentionally muted and minimized elements. The use of subtle patterns and the absence of any intricate details draw the eye’s attention to outliers that serve as guides while using the toys, while the rounded corners and edges created a visually cohesive and gentle appearance. The goal was for the toys to be inviting and self-explanatory, with additional features and game rules revealed through play and exploration.</p> <div class="img_row"> <img class="col three" src="/assets/monkey/gif/00-2.gif"/> </div> <p>Likewise, we also considered the time and labor constraints of the technicians and handlers who’d also work with the toys. To address this, we added intuitive quality of life features to make the toys more user-friendly and streamline their effort.</p> <h3 id="production-matters">Production Matters</h3> <p>We opted for the Prusa Mk3S 3D printers to manufacture the toys, enabling us to rapidly iterate on designs and materials. To enhance functionality, we upgraded the printers with 32-bit Raspberry Pi control boards and firmware, enabling remote printing, multi-printer management, and advanced control options. Additionally, we equipped the printers with tungsten coated heat breaks and nozzles which offer a heat-resistant and low-friction surface as a replacement for the typical PTFE tube.</p> <p>We experimented with various materials and infills (PLA, PETG, TPU, wood, PC) but in the end landed on PLA. PETG was also on the brittle side for our use-case. ABS and ASA were contenders but PLA ended up being more compatible. PLA can be recycled, is (industrially) compostable, but ultimately won out because our vendor could verify that it was food-safe.</p> <h3 id="future-ideas">Future Ideas</h3> <p>Looking ahead there are several areas I would’ve like to have focused on. First, I would’ve developed a traceable QR code that’d be embossed on each toy. These QR codes will contain crucial traceability information such as material source, time of manufacture, batch history, satefy and material data sheets, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/02_b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/02_b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/02_b-1400.webp"/> <img src="/assets/monkey/img/02_b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/01_b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/01_b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/01_b-1400.webp"/> <img src="/assets/monkey/img/01_b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Alternative designs for the toy underside </div> <p>Additionally, I wanted to invest in in-house recycling capabilities. By leveraging our own recycling machines we would be able to efficiently recycle toys on an individual, colony, or as per needed. Laboratories would effectively reserve a certain amount of plastic that can be shaped, cleaned, and reshaped according to their unique requirements. By providing enhanced transparency and accountability from the very beginning we provide a solid foundation for current and future scientific, industrial, and eco-political (ie. controls on plastic production and use) needs.</p>]]></content><author><name></name></author><category term="CAD"/><category term="neuroscience"/><category term="3D-printing"/><summary type="html"><![CDATA[Naxos Labs]]></summary></entry><entry><title type="html">Environment Sensor with Display</title><link href="https://xeroblaze0.github.io/blog/2021/enviroment-sensor/" rel="alternate" type="text/html" title="Environment Sensor with Display"/><published>2021-06-21T00:00:00+00:00</published><updated>2021-06-21T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2021/enviroment-sensor</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2021/enviroment-sensor/"><![CDATA[<p>Initially developed as a automation tool for gardening, this device monitors qualities of air, light, and soil in its environment.</p> <hr/> <h2 id="overview">Overview</h2> <p>The purpose of this device is to monitor its environment. It measures air pressure, temperature, humidity, volatile organic compounds (VOCs), ambient light luminosity, crude spectral analysis, and soil moisture. The exterior shell is 3D printed PLA, folded into its final shape. It’s controlled, and powered, using an Arduino Nano Sense BLE.</p> <h3 id="peripherals">Peripherals</h3> <p>The device takes advantage of several sensors, external and onboard, and a screen to display the information on. Below is a description of each:</p> <ul> <li><a href="https://learn.adafruit.com/adafruit-bme680-humidity-temperature-barometic-pressure-voc-gas/overview">BME680 Air Quality Sensor</a> <ul> <li>Temperature, pressure, humidity, VOCs</li> <li>I2C/SPI</li> <li>Requires calibration</li> </ul> </li> <li><a href="https://learn.adafruit.com/adafruit-tsl2591">TSL2591 HDR Light Sensor</a> <ul> <li>Luminosity</li> <li>I2C</li> <li>Contains IR diodes</li> </ul> </li> <li><a href="https://www.adafruit.com/product/1918">GUVA UV Sensor</a> <ul> <li>UVB/UVA (240-370nm)</li> <li>Analog</li> </ul> </li> <li><a href="https://www.adafruit.com/product/3595">ADPS9960 Light Sensor</a> <ul> <li>RGB color sensing</li> <li>Onboard Arduino Sense</li> <li>Contains IR detectors</li> </ul> </li> <li><a href="https://wiki.dfrobot.com/Capacitive_Soil_Moisture_Sensor_SKU_SEN0193">DFRobot Soil Sensor</a> <ul> <li>Capacitive moisture sensing</li> <li>Analog</li> <li>Requires calibration</li> </ul> </li> <li><a href="https://wiki.dfrobot.com/1.54%20Inches%20240_240_IPS_TFT_LCD_Display_with_MicroSD_Card_Breakout_SKU_DFR0649">ST7789 1.54” TFT Display</a> <ul> <li>240x240 pixels</li> <li>SPI communication</li> </ul> </li> </ul> <h3 id="enclosure">Enclosure</h3> <p>I decided to work with a folding assembly technique. This makes mounting the sensors easy in a small space while also simplifying the design. Below is an early shot of the foldable layout.</p> <p> <center><img src="/assets/enviro_sensor/img/foldy_face.png" width="80%;" height="100%;" alt=""/> <br/> <em>Figure 1: 3D model of enclosure</em></center> </p> <h2 id="control">Control</h2> <p>A few libraries need to be included for the sensors to operate. I found Adafruit’s display library easier to use than DFRobot’s. Some of the sensors need to be initialized beforehand, notably the VOC. It’s recommended that it run for 30 mins while in use. “This is because the sensitivity levels of the sensor will change during early use and the resistance will slowly rise over time as the MOX warms up to its baseline reading.” - <a href="https://learn.adafruit.com/adafruit-bme680-humidity-temperature-barometic-pressure-voc-gas/overview">Adafruit BME680 Overview</a>. For this reasion, the device has a 5 minute warm-up period before displaying VOC data.</p> <blockquote> <p>` bme.setGasHeater(320, 150); // 320*C for 150 ms`</p> </blockquote> <p>Likewise, the TSL2591 has a special initialization parameters:</p> <blockquote> <p>` void configureTSL2591(void){`</p> <p>` tsl.setGain(TSL2591_GAIN_MED); // 25x gain<code class="language-plaintext highlighter-rouge"> </code> tsl.setTiming(TSL2591_INTEGRATIONTIME_300MS);<code class="language-plaintext highlighter-rouge"> </code> tsl2591Gain_t gain = tsl.getGain();`</p> <p><code class="language-plaintext highlighter-rouge">}</code></p> </blockquote> <p>The controller assumes the TSL2591 and the APDS9960 have the same spectral response. First, the TSL2591 has two photodiodes: Visible spectrum + Infrared (VS+IR) response, and Infrared (IR) response. Subtracting the IR photodiode activity from the VS+IR activity leaves visible light. See Figure 2, left.</p> <p>Spectral responses are different, but similar enough to each other for this trick to work. The ADPS9960 might exhibit more activity towards the lowered end of the spectrum. Even though we’re strictly using the RGB channels, this will present a slight bias towards blue later in the project.</p> <div class="img_row"> <img class="col two" src="/assets/enviro_sensor/img/tsl2591_spectrum.png"/> <img class="col two" src="/assets/enviro_sensor/img/apds9960_spectrum.png" style="float: right"/> </div> <div class="col three caption"> Figure 2: left, TSL2591 spectral response; right, onboard APDS9960 spectral response </div> <p>The GUVA UV wavelength response is ~240-380nm, just out of reach of the APDS9960. Because of this, there’s no way to estimate how much luminosity is in the UV range<sup>*</sup>. This is a good excuse for a better UV sensor!</p> <h6>I didn't know of a way off hand and left it at that.</h6> <div class="img_row"> <img class="col three" src="/assets/enviro_sensor/img/guva_spectrum.png"/> </div> <div class="col three caption"> Figure 3: GUVA responsivity curve </div> <p>From here, we normalize color activity by dividing each color channel by the sum of all color channels:</p> <blockquote> <p>red % = r/(r+g+b)</p> <p>green % = g/(r+g+b)</p> <p>blue % = b/(r+g+b)</p> </blockquote> <p>We use these values with the visible light data to give us RGB luminosity data. With this and the IR luminosity data earlier, we can make a “full spectrum” wavelength response graph.</p> <p> <center><img src="/assets/enviro_sensor/img/screen.png" width="80%;" height="100%;" alt=""/> <br/> <em>Figure 4: Spectral data displayed on screen</em></center> </p> <p><br/> We’ll never know why I took screenshots instead of saving the original responsivity figures</p>]]></content><author><name></name></author><category term="arduino"/><category term="sensors"/><category term="embedded-systems"/><summary type="html"><![CDATA[An Arduino Project]]></summary></entry><entry><title type="html">MRI Compatible Actuator</title><link href="https://xeroblaze0.github.io/blog/2020/mri-actuator/" rel="alternate" type="text/html" title="MRI Compatible Actuator"/><published>2020-12-11T00:00:00+00:00</published><updated>2020-12-11T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/mri-actuator</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/mri-actuator/"><![CDATA[<p>Working with Dr. Netta Gurari in the Robotics and Sensorimotor Control Lab, our goal is to create and trace a sensory percept through the brain using an artificial stimulus and fMRI.</p> <p> <center> <a href="https://youtu.be/s4PYeK4FzfA"><img src="/assets/mri/thumb_yt.png" width="90%;" height="95%;"/></a> <em> Demonstration video</em> </center> </p> <hr/> <h2 id="abstract">Abstract</h2> <p>The purpose of this device is to deliver a tactile stimulus at an individual’s fingertip in a controlled manner in an MRI environment. The greater aim of this work is to use the apparatus to assess deficits in tactile perception, and where those deficits present themselves along the DCML pathway, up to and through the brain.</p> <h3 id="somatosensory-overview">Somatosensory Overview</h3> <p>Once a sensation is perceived, sensory information travels via Dorsal Column-Medial Lemniscal pathway, or via the anterolateral column (noxious/thermal)<sup>[2]</sup>. After traveling up the spinal column and reaching the medulla, the sensory information decussates, continuing up through the thalamus, and terminating at the somatosensory cortex.</p> <p> <center><img src="/assets/mri/dcml.png" width="80%;" height="100%;" alt=""/> <br/> <em>Figure 1: The dorsal column-medial lemniscus (DCML) pathway sends sensory and proprioceptive information. Above, lines indicate position of slices presented. Source: Kandel, Neural Science, 5th ed.</em></center> </p> <h3 id="touch-sensory-perception">Touch Sensory Perception</h3> <p>The dermis contains four mechanoreceptors, each with their own specialization<sup>[1]</sup>: Meissner Corpuscles for surface textures, Ruffini Corpuscles that react to skin stretch, and Pacinian Corpuscles are sensitive to vibrations and tool use.</p> <p>Merkel cells are sensitive to deep, static touches, and low vibrations (0-100Hz). They have a small receptive field and transduce detailed information about the surface they’re interacting with. Merkel cells signal the static aspect of a touch stimulus, such as pressure, whereas the terminal portions of the Merkel afferents in these complexes transduce the dynamic aspects of stimuli.[1]</p> <p> <center><img src="/assets/mri/receptors.png" width="100%;" height="100%;" alt=""/> <br/> <em>Figure 2: Receptors in the fingertip and the information they transduce Source: Purves, Neuroscience, 6th ed.</em></center> </p> <h3 id="stroke-overview">Stroke Overview</h3> <p>Strokes are caused by a disruption of blood flow to the brain, either by a clot in the blood vessel (ischemic stroke) or by a rupture (hemorrhagic stroke). Without oxygen or nutrients supplied by the blood stream cells begin to die. The scope of the damage is in part determined by the arterial network; neighboring branches of blood vessels may still be able to serve the affected area, limiting the potential damage. The same sort of stroke event can produce different effects between individuals.</p> <p> <center><img src="/assets/mri/ct.png" alt=""/> <br/> <em>Figure 3: CT scan of individual who expereinced stuttering as the result of a stroke in the left parietal lobe. Source: <a href="https://www.researchgate.net/figure/CT-scan-of-brain-showed-a-cortical-infarct-on-left-parietal-lobe-involving-Brodmann-area_fig1_7678181">Sahin et al. 2005</a></em></center> </p> <hr/> <h2 id="apparatus">Apparatus</h2> <h3 id="pneumatic-actuator">Pneumatic Actuator</h3> <p>To create the sensory signal I developed a small pneumatic piston that presses on the finger. Pneumatic systems use air as a working fluid. The piston is 3D printed using PLA polymer and is attached to an air compressor for pneumatic action. System is controlled using a PIC32 microcontroller and communicates serially with a computer. Two varaints of the piston were developed: a pneumatic return and a spring return. More information about the design can be found <a href="https://github.com/alexanderhay2020/499_pneumatic">here</a>.</p> <p> <center><img src="/assets/mri/gif/demo_test.gif" width="80%;" height="50%;" alt=""/> <br/> <em>Figure 4: User demo of the pneumatic actuator</em></center> </p> <h3 id="control">Control</h3> <p>Air is directed in and out of the piston using a solenoid controlled by the PIC. The solenoid has three (3) positions; allowing airflow to the cylinder, allowing airflow from the cylinder, and a neutral position that restricts all airflow.</p> <p> <center> <a href="https://www.lunchboxsessions.com/materials/flow-directional-control-valves/directional-control-valve-simulation"><img src="/assets/mri/gif/left-open.gif" width="40%;" height="100%;" style="margin-right:50px;"/></a> <a href="https://www.lunchboxsessions.com/materials/flow-directional-control-valves/directional-control-valve-simulation"><img src="/assets/mri/gif/right-open.gif" width="40%;" height="100%;"/></a> <br/> <em>Figure 5: Solenoid directing airflow. Graphic source: lunchboxsessions.com</em> </center> </p> <p>From there air is passed to a pressure regulator controlled by the PIC. Adjusting the pressure changes the speed and force in which the piston actuates.</p> <p>The PIC32 is a 32 bit, general purpose, microcontroller. It acts as the brains of the system, listening to the pressure sensors and directing the pressure regulator. It can also recognize when the probe has touched the fingertip. Real time system information is displayed on the screen and transmitted serially to a listening computer.</p> <p> <center> <img src="/assets/mri/gif/mri_sideXside_left.gif" width="40%;" height="100%;" style="margin-right:50px;"/> <img src="/assets/mri/gif/mri_sideXside_right.gif" width="40%;" height="100%;"/> <br/> <em>Figure 6: Actuating piston</em> </center> </p> <hr/> <h2 id="discussion--future-work">Discussion &amp; Future Work</h2> <p>During development I made it a priority that the device was MRI-compatible, which influenced many of the design considerations and challenges. The decision to use a pneumatic system made it so there is no interference from the apparatus during the MR imaging process. However, air is a compressible fluid which makes it difficult to control. A hydraulic system was considered to address that but was never implemented due to concerns about leakage.</p> <p>That said, leakage still proved to be a nuisance throughout the project. Pistons of various sizes were printed in an effort to dial in the tolerances, but ultimately will require better surface finishing. In addition to O-Rings, piston rod rings were incorporated into the design to alleviate the issue. Though not unsuccessful, it was not utilized in the final design.</p> <p>The control loop has two pressure sensors and one touch sensor. An optical linear encoder using fiber optic cables was considered, which would’ve provided more certain feedback of the piston, but ultimately was shelved due to time constraints.</p> <p>Moving forward, our project will focus on enhancing the robustness of each component and establishing a solid framework for development, as well as actuator characterization and precise interaction with the finger. Currently, our controller leverages the CTMU module on the PIC32. To ensure its functionality, the surface that the fingertip comes into contact with must be both conductive and MRI-compatible, such as aluminum foil. Studies examining the heating effects of MRI scanning have shown that no significant heating (burning) occurs with nonferromagnetic materials.<sup><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.1910070302?sid=nlm%3Apubmed">[2]</a></sup> There is also no risk of interference caused by motion<sup><a href="https://pubmed.ncbi.nlm.nih.gov/18982643/">[3]</a></sup>.</p> <h2 id="more-information">More Information</h2> <p>More information and details specific to the project, please see my Github link <a href="https://github.com/alexanderhay2020/499_pneumatic">here</a>.</p> <h2 id="references">References</h2> <p>[1] Purves, D., Augustine, G., Fitzpatrick, D., Hall, W., LaMantia, A., Mooney, R., Platt, M. and White, L., 2018. Neuroscience. 6th ed.</p> <p>[2] Kandel, E. R., 2013. Principles of Neural Science. 5th ed.</p> <p>[3] Buchli R, Boesiger P, Meier D. Heating effects of metallic implants by MRI examinations. Magn Reson Med. 1988;7(3):255-261. doi:10.1002/mrm.1910070302</p> <p>[4] Fischer GS, Krieger A, Iordachita I, Csoma C, Whitcomb LL, Gabor F. MRI compatibility of robot actuation techniques–a comparative study. Med Image Comput Comput Assist Interv. 2008;11(Pt 2):509-517. doi:10.1007/978-3-540-85990-1_61</p>]]></content><author><name></name></author><category term="PIC"/><category term="C++"/><category term="3D-printing"/><category term="CAD"/><category term="embedded-systems"/><summary type="html"><![CDATA[Artificial Tactile Stimulation]]></summary></entry><entry><title type="html">Microscopy and Image Analysis</title><link href="https://xeroblaze0.github.io/blog/2020/microscopy/" rel="alternate" type="text/html" title="Microscopy and Image Analysis"/><published>2020-06-09T00:00:00+00:00</published><updated>2020-06-09T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/microscopy</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/microscopy/"><![CDATA[<h3 id="fiji">FIJI</h3> <p><a href="https://imagej.net/Fiji">FIJI</a> is an open source image analysis tool for the scientific community. This post demos how to use some of the tools FIJI has, including data collection techniques.</p> <h3 id="initial-data">Initial Data</h3> <div class="img_row"> <img class="col three" src="/assets/fiji/SACs_color.png"/> </div> <div class="col three caption"> Figure 1: Original image </div> <p>The image for this demo is of starburst amacrine cells (SACs), interneurons in the retina. The image is a single channel .tif, 512x512 pixels, with each pixel representing 0.62 microns. With this information we definitively measure how big each cell is.</p> <h4 id="brightness--contrast">Brightness &amp; Contrast</h4> <p>The first thing I do is adjust the brightness and contrast. By looking at the histogram we can see that the image is very dark, more dark than it needs to be. Lowering the maximum value fills out the range of present colors. The difference is clear in Figure 2.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_02a-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_02a-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_02a-1400.webp"/> <img src="/assets/fiji/fig_02a.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_02b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_02b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_02b-1400.webp"/> <img src="/assets/fiji/fig_02b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: left, unaltered maximum; right, fitted maximum </div> <p>It’s important in this step to not over-saturate the image by raising or lowering the minimum/maximum values. This step is to present the information within the image as a whole. Tweaking and refining will come later.</p> <p>For this demo I want to look at the cells, not the nebulous background or nodelets. Given that, the image doesn’t need to be as bright. However, it’s important to note that brightening the image revealed more cells and provided better definition. Where this middle ground is depends on the observer and whatever their interests are.</p> <h4 id="threshold">Threshold</h4> <p>Thresholding is an important first step in many image analysis techniques. It converts the image to a black and white image. Here I adjust the histogram in the same fashion as before and focus it around the peak. That filters out the background while keeping true to the cell size and shape.</p> <div class="img_row"> <img class="col three" src="/assets/fiji/fig_03.png"/> </div> <div class="col three caption"> Figure 3: Initial image after apply the threshold. Note, adjusting for a dark background flips the histogram values </div> <p>Sometimes after adjusting the image, the subject matter may blend together. In this case applying the threshold created some peanut shaped objects. Watershed segmentation addresses the issue and cuts the “peanut” in half. Figure 4 highlights the effect.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_04a-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_04a-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_04a-1400.webp"/> <img src="/assets/fiji/fig_04a.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_04b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_04b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_04b-1400.webp"/> <img src="/assets/fiji/fig_04b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Left, pre-watershed; Right, post-watershed </div> <h4 id="analyzing-the-image">Analyzing the Image</h4> <p>This is where the magic happens. Once the cells are properly individualized we can count and measure each cell. FIJI automatically does this. Since I’m interested in the size and position of the cells I’ve excluded any cells that are cropped by the edge. From here the data is exported to a .csv file.</p> <div class="img_row"> <img class="col three" src="/assets/fiji/fig_05.png"/> </div> <div class="col three caption"> Figure 5: FIJI can automatically count and size each cell. </div> <h3 id="matlab">MATLAB</h3> <p>The exported data contains a cell ID, area, and x/y position. First, I want to determine if the size of the cells fits any distribution. The Jarque-Bera Test tests the normality of a dataset and MATLAB has a built in function for this. Applying the JB test to the data gives a result of 1, meaning that the test rejects the hypothesis that the data (cell size) is normally distributed.</p> <p>We can also look at the nearest neighbor of each cell (see Figure 6).</p> <div class="img_row"> <img class="col three" src="/assets/fiji/fig_06.png"/> </div> <div class="col three caption"> Figure 6: Looking at each cell's nearest neighbor </div> <p>So the size of the cells aren’t interesting, but what about the spatial distribution of the cells? Using the data provided by the image, we can calculate the coefficient of variation:</p> <p>μ = 21.0440</p> <p>σ = 5.3968</p> <p>CoV = σ/μ = 0.26</p> <p>(units are in microns)</p> <p>Defining λ as the average distance of the nearest neighbor (21.0440), the Poisson coefficient of variation is:</p> <p>CoV = λ^(-1/2) = 0.22</p> <p>A Poisson distribution assumes that cells are “blind” to each other’s positions as they develop and can’t occupy the same space. The fact that these coefficients are close infers that these SACs are also blind as they develop.</p>]]></content><author><name></name></author><category term="biology"/><category term="neuroscience"/><category term="stats"/><category term="matlab"/><summary type="html"><![CDATA[Using FIJI/ImageJ to analyze cell populations]]></summary></entry><entry><title type="html">Principal Component Analysis</title><link href="https://xeroblaze0.github.io/blog/2020/principal_component_analysis/" rel="alternate" type="text/html" title="Principal Component Analysis"/><published>2020-05-13T00:00:00+00:00</published><updated>2020-05-13T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/principal_component_analysis</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/principal_component_analysis/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/408/blob/master/hw/hw5/homework5.m"><div class="color-button">GitHub</div></a> </p> <h3 id="neuron-anatomy">Neuron Anatomy</h3> <p>Neurons generally have four functional regions; input, integration, conduction, and output. Inputs are generated current flowing in and out of the cell. Inputs are aggregated, and if triggered, generate an action potential and releasing neurotransmitters. In this exercise I examine the intracellular activity of a cell and determine how many presynaptic cells are providing an input, as well as the activity level of each input.</p> <div class="img_row"> <img class="col three" src="/assets/pca/fig_01.png"/> </div> <div class="col three caption"> Figure 1: General functional regions of the neuron </div> <h3 id="initial-data">Initial Data</h3> <p>To visualize the data I plotted it as a heat map. The left image shows all of the data, the right image displays fewer samples, highlighting the different inputs the cell is receiving.</p> <div class="img_row"> <img class="col three" src="/assets/pca/fig_02.png"/> </div> <div class="col three caption"> Figure 2: left, all of the sample data plotted; right, samples showing different responses </div> <p>The data is a series of voltage measurements over time; if we look at a covariance matrix (Figure 3) it would be able to show us how the voltage measured at each time point vary together.</p> <div class="img_row"> <img class="col three" src="/assets/pca/fig_03.png"/> </div> <div class="col three caption"> Figure 3: Covariance matrix of the cell voltage data </div> <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3> <p>From there we can run PCA on the data, seen in Figure 4. Three PCs stand out in that they explain more fractional variance than the other PCs, but ultimately we would need hundreds to explain all of the data. By plotting those three principal components we can clearly see three signal responses (Figure 5, left). Looking at the histogram (Figure 5, right) confirms that the 4th PC has a gaussian centered at 0, a strong indicator of noise.</p> <div class="img_row"> <img class="col" src="/assets/pca/fig_04.png"/> </div> <div class="col three caption"> Figure 4: Principal components plotted as percent of variance explained </div> <div class="img_row" style="margin-right:1.5rem; margin-left:1.5rem;"> <img class="col two" style="float:left; padding-right: 1rem;" src="/assets/pca/fig_05_l.png"/> <img class="col two" style="float:right; padding-left: 1rem;" src="/assets/pca/fig_05_r.png"/> </div> <div class="col three caption"> Figure 5: Left, signal response of principal components 1-3; right, histogram scores </div> <h3 id="k-means-classification-and-event-identification">K-means Classification and Event Identification</h3> <p>Now that we have a waveform with which to use, we can use a classifier. In this exercise I use the K-means tool in MATLAB to comb through the data and ‘classify’ the inputs based on the PCA, counting each time a synapse event occurs. In this example each event occurred 587, 108, and 127 times respectively. There’s a number of guides and videos on how K-means works.</p> <p><img class="col three" src="/assets/pca/fig_06.png"/> </p> <div class="col three caption"> Figure 6: Results of K-means classification </div>]]></content><author><name></name></author><category term="math"/><category term="stats"/><category term="neuroscience"/><category term="matlab"/><summary type="html"><![CDATA[Using PCA to determine number of presynaptic inputs of a cell]]></summary></entry><entry><title type="html">Shape Memory Alloy Actuation</title><link href="https://xeroblaze0.github.io/blog/2020/shape-memory-alloy/" rel="alternate" type="text/html" title="Shape Memory Alloy Actuation"/><published>2020-03-17T00:00:00+00:00</published><updated>2020-03-17T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/shape-memory-alloy</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/shape-memory-alloy/"><![CDATA[<p>The goal of this project was to explore the use of shape memory alloys to mimic human movement using equilibrium point control. <a href="https://youtu.be/f2VmKv2i89Y">YouTube</a></p> <h3 id="shape-memory-alloys">Shape Memory Alloys</h3> <p>Shape memory alloys (SMA) operate because of the unique crystalline structure of the alloy. The actuators used in this project are made with Nitinol, an SMA with a 1:1 Nickel-Titanium ratio. These alloys have a transition temperature and easily deform when below this temperature. The crystalline structure shifts but does not suffer permanent damage (C-&gt;A). Once the wire is heated above the transition temperature the crystals undergo a phase change and revert back to their original structure, with a large force (A-&gt;B). When the wire cools again the phase change reverses and the wire can be deformed again (B-&gt;C).</p> <p> <center><img src="/assets/sma/Figure_3-2.jpg" width="50%;" height="50%;" alt=""/> <br/> <em>Diagram showing how SMAs atomic structure deforms</em></center> </p> <p>As a wire, this deformation manifests as a change of length in the wire. For the actuators to actuate, a current is passed through the wire. The internal resistance of the wire heats it up above its transition temperature, instigating the phase change and <em>contraction</em> and creating a pulling force.</p> <h3 id="equilibrium-point-control">Equilibrium Point Control</h3> <p>Equilibrium point control (EPC) is motor control through changing muscle stiffness. A ‘reciprocal command’ is sent to shift the equilibrium point, moving the limb. In the λ‐model of motor control, proposed by Feldman<sup><a href="https://www.ncbi.nlm.nih.gov/pubmed/15136283?dopt=Abstract">[1]</a><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2342993/">[2]</a></sup>, the reciprocal command, alters the relative activation of agonist and antagonist muscles to produce a shift in the equilibrium between the muscle force and external load. While it has fallen out of vogue in human motor control, this project explores using it as a means of actuation of SMAs.</p> <p> <center><img src="/assets/sma/alpha_model.png" width="100%;" height="100%;" alt=""/> <br/> <em>α model: motor control through changing muscle stiffness</em></center> </p> <h3 id="actuator">Actuator</h3> <p>The Miga T220 SMA linear actuator was chosen for this project. Other considerations included servo motors, magnetic actuators, belt/gear mechanical actuators, pneumatic systems, and other SMA actuators. Form, power, and hardware interface were the driving deciding factors. A decision matrix can be found <a href="https://docs.google.com/spreadsheets/d/1p3fjq4K-Gl2H-Soi13WV8KOYeHxsrkHLGmXQxgn4opM/edit?usp=sharing">here</a></p> <p> <center> <img src="/assets/sma/miga.jpg" width="70%;" height="70%;" alt=""/> <br/> <em>Miga T220 SMA Linear Actuator</em> </center> </p> <h3 id="design-and-fabrication">Design and Fabrication</h3> <p>The project served as a study in rapid prototyping. I used Solidworks for part models, assembly, and drawings. The frame was built from acrylic cut from a laser cutter, special screws and couplings were printed using the Ultimaker3 3D printer. Plastic hardware bits are required for stacking the actuators, as the mounting holes are part of the circuit.</p> <p>For the presentation at the Museum of Science and Industry, two (2) of the Miga actuators were removed and the fans repositioned to directly face the actuators. These changes allows faster actuation time at the cost of actuation force.</p> <p> <center><img src="/assets/sma/assembly.gif" width="50%;" height="50%;" alt=""/> <br/> <em>Apparatus exploded view</em></center> </p> <h3 id="controller">Controller</h3> <p>See my <a href="https://github.com/alexanderhay2020/499">GitHub</a> page.</p> <h3 id="discussion">Discussion</h3> <p>The project as presented demonstrates an α-model method of motor control through changing the Nitinol wire properties. λ-model requires a feedback signal from the muscle spindle, a fiber running the length of the muscle that senses stretch sensory information. A λ-model could be modeled by installing a position sensor on the actuator or a rotary encoder at the joint, sending the muscle property data that the muscle spindle broadcasts. A PID controller could then be implemented to give the actuators a spring-like quality that is seen in muscle fibers<sup><a href="https://www.ncbi.nlm.nih.gov/pubmed/8930238">[3]</a></sup>.</p> <p>Future work would also involve building out the “forearm”. For now the apparatus actuates a collar with a Base-15 u channel, seen with a bolt in place in the demo video.</p> <p> <center> <img src="/assets/sma/img01.jpg" width="80%;" height="80%;" alt=""/> <br/> <em>Complete assembly of project</em> </center> </p> <h3 id="references">References</h3> <p>[1] Anatol G. Feldman (1986) Once More on the Equilibrium-Point Hypothesis (λ Model) for Motor Control, Journal of Motor Behavior, 18:1, 17-54, DOI: 10.1080/00222895.1986.10735369</p> <p>[2] Hinder, Mark R, and Theodore E Milner. “The case for an internal dynamics model versus equilibrium point control in human movement.” The Journal of physiology vol. 549,Pt 3 (2003): 953-63. doi:10.1113/jphysiol.2002.033845</p> <p>[3] Gribble PL, Ostry DJ. Origins of the power law relation between movement velocity and curvature: modeling the effects of muscle mechanics and limb dynamics. J Neurophysiol. 1996 Nov;76(5):2853-60. doi: 10.1152/jn.1996.76.5.2853. PubMed PMID: 8930238.</p>]]></content><author><name></name></author><category term="arduino"/><category term="control"/><category term="embedded-systems"/><summary type="html"><![CDATA[Equilibrium Point Control]]></summary></entry><entry><title type="html">Function Approximation Using Radial Basis Functions</title><link href="https://xeroblaze0.github.io/blog/2020/radial_basis_functions/" rel="alternate" type="text/html" title="Function Approximation Using Radial Basis Functions"/><published>2020-01-15T00:00:00+00:00</published><updated>2020-01-15T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/radial_basis_functions</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/radial_basis_functions/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/469_bme/blob/master/ps1/py/part1.py"><div class="color-button">GitHub</div></a> </p> <h3 id="color-specific-photoreceptors---cones">Color Specific Photoreceptors - Cones</h3> <p>Inside the retina are cone cells, photosensitive cells that differentiate color. Humans have 3 different types of cones; (S)mall, (M)edium, and (L)arge, corresponding to the length of the wavelength that excites it. The excitement amplitudes of each type of cone is perceived to us as color, and the color perceived is the sum of each cone response.</p> <div class="img_row"> <img class="col three" src="/assets/rbf/figure_5.gif"/> </div> <div class="col three caption"> The color perceived is the sum of each cone response </div> <h3 id="radial-basis-functions">Radial Basis Functions</h3> <p>A Radial Basis Function (RBFs) is a function whose value depends the distance between a query point and a fixed point. For this exercise I used the Gaussian Function:</p> \[h(x)=exp(-\frac{(x-c^2)}{r^2})\] <ul> <li><em>x</em> is the query point</li> <li><em>c</em> is some fixed point, 0 if distance is measured from origin</li> <li><em>h(x)</em> is the RBF</li> </ul> <p>By using multiple RBFs you can approximate a function. By multiplying the RBF by some weight, summing a network of RBFs can approximate a function:</p> \[f(x) = \sum_{j=1}^{m} w_j h_j(x)\] <ul> <li><em>h(x)</em> is the RBF</li> <li><em>w</em> is the weight for the RBF</li> <li><em>j</em> in the index for <em>m</em> samples of x</li> </ul> <p>The weight vector can be found using linear regression, ultimately leading to this equation:</p> \[\overrightarrow{w} = (H^TH)^{-1}H^T\overrightarrow{y}\] <ul> <li><em>H</em> is the <em>design matrix</em> of <em>h(x)</em>, or a <em>nxm</em> matrix of <em>n</em> samples and <em>m</em> RBFs</li> <li><em>y</em> is f(x) vectorized</li> </ul> <p>In this exercise we have the following dataset:</p> <ul> <li><em>x</em> is drawn from a uniform random distribution, from <em>-10 &lt; n &lt; 10; n=1,000</em></li> <li><em>y = 2x + e</em>; e is a normally distributed noise vector, $μ = 1, σ = 0$</li> <li>Use 48 RBFs, between -12 and 12 @ every 0.5 along the x axis</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_1-1400.webp"/> <img src="/assets/rbf/Figure_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_2-1400.webp"/> <img src="/assets/rbf/Figure_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_3-1400.webp"/> <img src="/assets/rbf/Figure_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_4-1400.webp"/> <img src="/assets/rbf/Figure_4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_1.png">Fig. 1</a> - Dataset visualized</p> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_2.png">Fig. 2</a> - 48 RBFs plotted</p> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_3.png">Fig. 3</a> - Function prediction</p> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_4.png">Fig. 4</a> - Error analysis</p> <p>Modeling photoreceptor response provides insight to how information is gathered and processed at the cellular level. It’s the network of these cone cells that provide the stimulus we interpret as color.</p>]]></content><author><name></name></author><category term="math"/><category term="stats"/><category term="biology"/><category term="matlab"/><summary type="html"><![CDATA[Using radial basis functions to approximate a linear mapping]]></summary></entry><entry><title type="html">Using Baxter for Object Recognition and Manipulation</title><link href="https://xeroblaze0.github.io/blog/2019/baxter-manipulation/" rel="alternate" type="text/html" title="Using Baxter for Object Recognition and Manipulation"/><published>2019-12-13T00:00:00+00:00</published><updated>2019-12-13T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2019/baxter-manipulation</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2019/baxter-manipulation/"><![CDATA[<p>The goal for the project was to use Baxter to recognize a nerf gun, pick it up and fire it at a coffee mug. At each step Baxter asks the user for confirmation before moving to the next sequence in the task. <a href="https://github.com/ME495-EmbeddedSystems/final-project-terminator">GitHub</a><br/></p> <h3 id="overall-system-architecture-and-high-level-concepts">Overall System Architecture and High Level Concepts</h3> <p>We made Baxter a high level state machine to navigate the task. The state machine (commander node) coordinated his left arm, right arm, cameras, and image processing. We determined that the left arm would find, pick up, and aim the gun, and the right arm would pull the trigger. AprilTags were used to identify the inital position of the nerf gun using the cameras on Baxter’s wrists.</p> <div class="img_row"> <img class="col three" src="/assets/baxter/baxter_01.png"/> </div> <div class="col three caption"> Baxter </div> <p>We used the move_it kinematics library to manipulate both of Baxter’s arms. Baxter’s wrists/cameras presented a unique challenge because their rotation is not quite 360°, with the midpoint of the rotation placing the camera underneath the gripper. Since the his arm and camera would be in line with the nerf gun we wanted to use the wrist camera to aim (the only other options were his face camera and the camera on his other arm). Baxter also has a “safety bubble” that prevents the arms from colliding into the body. With the nerf gun in hand the trigger ended up falling into the safety bubble.</p> <p>To get around these challenges we flipped the gun upside-down, then modeled and printed a riser to fit in the nerf gun’s rail and a set of custom grippers. The riser did two things; it created a larger surface area for Baxter’s grippers and raised the gun out of the camera’s field of view. Without the riser and grippers the nerf gun occluded the camera nearly 50% including the target.</p> <div class="img_row"> <img class="col three" src="/assets/baxter/baxter_02.png"/> </div> <div class="col three caption"> Point of view from Baxter's wrist camera </div> <p>Once Baxter had the nerf gun he looked for the target mug then aimed the nerf gun. The darknet_ros library was used for object recognition, with images fed from the wrist camera. Despite not having to train the classifier, darknet_ros was very slow running in real time, so aiming was the longest operation. The aiming task was deemed complete once the mug was in the middle of the camera’s field of view.</p> <p>While the riser did lift the trigger out of the safety bubble, Baxter’s movement precision using the stock grippers was not enough the “thread the needle” in the space needed to pull the trigger. Swapping the stock grippers for the long ones resolved the issue, and when they were equipped with the rubber tips we were able to achieve a consistent trigger pull.</p> <div class="img_row"> <img class="col three" src="/assets/baxter/terminator.gif"/> </div> <div class="col three caption"> Dishing the Law </div> <h3 id="future-work">Future Work</h3> <p>We would like to implement tracking and shooting of objects that move through space. This would require a more robust system than Darknet since the update rate was extremely slow. We could potentially also take out objects we aren’t detecting in Darknet. For stationary objects, we could implement a planning algorithm that once the camera detects the desired object, we would no longer have to run Darknet or other object detection algorithms which would speed up the time it takes between first detecting an object and being lined up to shoot it.</p> <p><a href="https://youtu.be/2MRsNefNWmw">Here</a> is a link to the final video showing Baxter in action.</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="robots"/><category term="control"/><category term="python"/><category term="embedded-systems"/><summary type="html"><![CDATA[Embedded Systems Final Project]]></summary></entry><entry><title type="html">Computing Logic Funcitons using Perceptrons</title><link href="https://xeroblaze0.github.io/blog/2019/perceptrons/" rel="alternate" type="text/html" title="Computing Logic Funcitons using Perceptrons"/><published>2019-11-20T00:00:00+00:00</published><updated>2019-11-20T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2019/perceptrons</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2019/perceptrons/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/"><div class="color-button">GitHub</div></a> </p> <h3 id="emulated-neurons">Emulated Neurons</h3> <p>Neural networks are built on units called neurons, and for this exercise a special neuron called a perceptron is used. Perceptrons are special in that they can represent fundamental logic functions: AND, OR, NAND, NOR. Though a perceptron can’t represent XAND or XOR, layered perceptrons can, thus all logic functions can potentially be built using a layered network structure.</p> <p> <img src="/assets/perceptron/img/nn_01.png" width="511" height="286" alt=""/> <br/> <em><a href="https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182">images</a> showing perceptrons' logic structure</em> </p> <p>Perceptrons work by multiplying a vector of inputs by a weight vector and passing the sum of that input-weight vectors through an activation function. For this exercise I used the sigmoid function, but there are many others. Weights are [nxm] matrices, where n is the dimension of the input and m is the dimension of the output.</p> <p> <img src="/assets/perceptron/img/nn_02.png" alt=""/> <br/> <em> image showing perceptron model</em> </p> <p><br/></p> <p>Here is a sketch algorithm to implement a perceptron node:</p> <p><br/></p> <p>\(\Sigma (x_iw_i) = x_1w_1 + x_2w_2 + ... + x_nw_n\) \(\sigma = \frac{1}{1+e^{\Sigma (x_iw_i )}}\) <br/></p> <ul> <li><em>x</em> is the sample input</li> <li><em>w</em> is the the associated weight for the input sample</li> </ul> <p>For the perceptron to work properly, the weights need to be adjusted according to the desired output. To calculate and adjust the error we first subtract the predicted output from the actual output.</p> <p>\(\epsilon=y-\sigma\) <br/></p> <ul> <li><em>ϵ</em> is the error</li> <li><em>y</em> is the acutal output</li> <li><em>σ</em> is defined above</li> </ul> <p>Using gradient descent, we find the adjustment needed for the weights by computing the derivative of the sigmoid function and multiplying that by the error to give us the final adjustment for the weights:</p> <p>\(\sigma' = \sigma (1- \sigma)\) <br/></p> <ul> <li><em>σ’</em> is the sigmoid derivative when given σ as above</li> </ul> <p>\(adjustment = \epsilon*\sigma'\) <br/></p> \[w_i=w_i+ \hat{x}^T \cdot adjustments\] <p>Networked together, perceptrons can be immensely powerful and are the foundations by which many neural nets are built. These new weights wouldn’t have changed much, but over many iterations they converge to their proper values of minimizing error. This method of adjusting the weights is called backpropagation.</p> <p>To test the algorithm a small, simple sample set was used to provide easy-to-interpret results. The table below shows the following dataset such that the output is 1 if first or second columns contained a 1, disregrading the third column:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> </tr> </tbody> </table> <p><a href="/assets/perceptron/py/perceptron.py">perceptron.py</a> demonstrates the algorithm and predicted output. Given the input array and initial weights adjusted 200​ times, the predicted results are as follows:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0.135</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>0.999</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>0.917</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>0.917</td> </tr> </tbody> </table> <p>Given an infinite number of iterations the algorithm would converge to either 0 or 1, but in 200 iterations our results are close enough to see a clear distinction.</p> <p>Applied to a larger dataset, <a href="/assets/perceptron/py/classifier.py">classifier.py</a>, we can create a linear classifier.</p> <p> <img src="/assets/perceptron/img/Figure_2-1.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/img/Figure_2-2.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial 2D dataset, Right: Perceptron classifier results</em> </p> <p> <img src="/assets/perceptron/img/Figure_2-4.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/img/Figure_2-5.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial validation dataset, Right: Perceptron validation classifier results</em> </p> <p>The graph below shows the network error over 500 iterations. As expected the initial error is very high due to the weights being initially random. The error quicky drops after ~30 iterations, but never quite reaches zero. In this case error is ~4%, reflected in the misclassifed samples in both images on the right.</p> <p> <img src="/assets/perceptron/img/Figure_2-3.png" width="50%;" height="50%;" alt=""/> <br/> <em>Network error percentage drops after each epoch, indicating a model is being learned</em> </p>]]></content><author><name></name></author><category term="nerual-net"/><category term="machine-learning"/><category term="python"/><summary type="html"><![CDATA[Using layered perceptrons to compute logic functions]]></summary></entry></feed>