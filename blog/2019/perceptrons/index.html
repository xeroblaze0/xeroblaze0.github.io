<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Computing Logic Funcitons using Perceptrons | Alex Hay</title> <meta name="author" content="Alex Hay"/> <meta name="description" content="Using layered perceptrons to compute logic functions"/> <meta name="keywords" content="robotics, neuroscience, engineering, mechanics"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xeroblaze0.github.io/blog/2019/perceptrons/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span>Alex&nbsp;</span>Hay</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/blog/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Computing Logic Funcitons using Perceptrons</h1> <p class="post-meta">November 20, 2019</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a> &nbsp; &middot; &nbsp; <a href="/blog/tag/nerual-net"> <i class="fas fa-hashtag fa-sm"></i> nerual-net</a> &nbsp; <a href="/blog/tag/machine-learning"> <i class="fas fa-hashtag fa-sm"></i> machine-learning</a> &nbsp; <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a> &nbsp; </p> </header> <article class="post-content"> <div id="markdown-content"> <p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/"><div class="color-button">GitHub</div></a> </p> <h3 id="emulated-neurons">Emulated Neurons</h3> <p>Neural networks are built on units called neurons, and for this exercise a special neuron called a perceptron is used. Perceptrons are special in that they can represent fundamental logic functions: AND, OR, NAND, NOR. Though a perceptron can’t represent XAND or XOR, layered perceptrons can, thus all logic functions can potentially be built using a layered network structure.</p> <p> <img src="/assets/perceptron/img/nn_01.png" width="511" height="286" alt=""/> <br/> <em><a href="https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182">images</a> showing perceptrons' logic structure</em> </p> <p>Perceptrons work by multiplying a vector of inputs by a weight vector and passing the sum of that input-weight vectors through an activation function. For this exercise I used the sigmoid function, but there are many others. Weights are [nxm] matrices, where n is the dimension of the input and m is the dimension of the output.</p> <p> <img src="/assets/perceptron/img/nn_02.png" alt=""/> <br/> <em> image showing perceptron model</em> </p> <p><br/></p> <p>Here is a sketch algorithm to implement a perceptron node:</p> <p><br/></p> <p>\(\Sigma (x_iw_i) = x_1w_1 + x_2w_2 + ... + x_nw_n\) \(\sigma = \frac{1}{1+e^{\Sigma (x_iw_i )}}\) <br/></p> <ul> <li><em>x</em> is the sample input</li> <li><em>w</em> is the the associated weight for the input sample</li> </ul> <p>For the perceptron to work properly, the weights need to be adjusted according to the desired output. To calculate and adjust the error we first subtract the predicted output from the actual output.</p> <p>\(\epsilon=y-\sigma\) <br/></p> <ul> <li><em>ϵ</em> is the error</li> <li><em>y</em> is the acutal output</li> <li><em>σ</em> is defined above</li> </ul> <p>Using gradient descent, we find the adjustment needed for the weights by computing the derivative of the sigmoid function and multiplying that by the error to give us the final adjustment for the weights:</p> <p>\(\sigma' = \sigma (1- \sigma)\) <br/></p> <ul> <li><em>σ’</em> is the sigmoid derivative when given σ as above</li> </ul> <p>\(adjustment = \epsilon*\sigma'\) <br/></p> \[w_i=w_i+ \hat{x}^T \cdot adjustments\] <p>Networked together, perceptrons can be immensely powerful and are the foundations by which many neural nets are built. These new weights wouldn’t have changed much, but over many iterations they converge to their proper values of minimizing error. This method of adjusting the weights is called backpropagation.</p> <p>To test the algorithm a small, simple sample set was used to provide easy-to-interpret results. The table below shows the following dataset such that the output is 1 if first or second columns contained a 1, disregrading the third column:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> </tr> </tbody> </table> <p><a href="/assets/perceptron/py/perceptron.py">perceptron.py</a> demonstrates the algorithm and predicted output. Given the input array and initial weights adjusted 200​ times, the predicted results are as follows:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0.135</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>0.999</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>0.917</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>0.917</td> </tr> </tbody> </table> <p>Given an infinite number of iterations the algorithm would converge to either 0 or 1, but in 200 iterations our results are close enough to see a clear distinction.</p> <p>Applied to a larger dataset, <a href="/assets/perceptron/py/classifier.py">classifier.py</a>, we can create a linear classifier.</p> <p> <img src="/assets/perceptron/img/Figure_2-1.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/img/Figure_2-2.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial 2D dataset, Right: Perceptron classifier results</em> </p> <p> <img src="/assets/perceptron/img/Figure_2-4.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/img/Figure_2-5.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial validation dataset, Right: Perceptron validation classifier results</em> </p> <p>The graph below shows the network error over 500 iterations. As expected the initial error is very high due to the weights being initially random. The error quicky drops after ~30 iterations, but never quite reaches zero. In this case error is ~4%, reflected in the misclassifed samples in both images on the right.</p> <p> <img src="/assets/perceptron/img/Figure_2-3.png" width="50%;" height="50%;" alt=""/> <br/> <em>Network error percentage drops after each epoch, indicating a model is being learned</em> </p> </div> </article></div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> &copy; Copyright 2023 Alex Hay. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Last updated: August 02, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>